{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model Klasifikasi\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluasi Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style untuk visualisasi\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Library import successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# UTILITIES: EVALUATION FUNCTION\n",
    "# ===========================\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion_Matrix': cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.1 LOAD DATA DAN EKSPLORASI AWAL\n",
    "# ===========================\n",
    "\n",
    "# Load dataset dari Kaggle\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "csv_path = f\"{path}/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "# Baca dataset\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EKSPLORASI AWAL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Tampilkan 5 baris pertama\n",
    "print(\"\\n1Ô∏è‚É£ Lima Baris Pertama Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Info Dataset\n",
    "print(\"\\n2Ô∏è‚É£ Informasi Dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# 3. Statistik Deskriptif\n",
    "print(\"\\n3Ô∏è‚É£ Statistik Deskriptif:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\nüìä Dimensi Dataset: {df.shape[0]} baris, {df.shape[1]} kolom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.2 IDENTIFIKASI MISSING VALUE\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IDENTIFIKASI MISSING VALUE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Hitung missing value\n",
    "missing_data = pd.DataFrame({\n",
    "    'Kolom': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "if len(missing_data) == 0:\n",
    "    print(\"\\n‚úì Tidak ada missing value dalam dataset!\")\n",
    "else:\n",
    "    print(\"\\nMissing Value Ditemukan:\")\n",
    "    print(missing_data)\n",
    "    \n",
    "    # Visualisasi missing value\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    missing_data_plot = df.isnull().sum()\n",
    "    missing_data_plot = missing_data_plot[missing_data_plot > 0].sort_values(ascending=False)\n",
    "    missing_data_plot.plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Jumlah Missing Value')\n",
    "    ax.set_title('Distribusi Missing Value per Kolom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946aefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.2B IDENTIFIKASI DAN HAPUS DUPLICATE DATA\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IDENTIFIKASI DAN HAPUS DUPLICATE DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cek duplicate berdasarkan semua kolom\n",
    "duplicate_all = df.duplicated().sum()\n",
    "print(f\"\\n1Ô∏è‚É£ Duplikat (semua kolom): {duplicate_all} baris\")\n",
    "\n",
    "# Cek duplicate berdasarkan customerID (unique identifier)\n",
    "if 'customerID' in df.columns:\n",
    "    duplicate_id = df.duplicated(subset=['customerID'], keep=False).sum()\n",
    "    print(f\"2Ô∏è‚É£ Duplikat berdasarkan customerID: {duplicate_id} baris\")\n",
    "    \n",
    "    if duplicate_id > 0:\n",
    "        print(\"\\n   Menampilkan duplikat customerID:\")\n",
    "        dup_customers = df[df.duplicated(subset=['customerID'], keep=False)].sort_values('customerID')\n",
    "        print(dup_customers[['customerID', 'tenure', 'MonthlyCharges']].head(10))\n",
    "\n",
    "# Hapus duplikat\n",
    "print(\"\\n3Ô∏è‚É£ Menghapus duplikat...\")\n",
    "df_before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "df_after = len(df)\n",
    "rows_removed = df_before - df_after\n",
    "\n",
    "print(f\"   Baris sebelum: {df_before}\")\n",
    "print(f\"   Baris sesudah: {df_after}\")\n",
    "print(f\"   ‚úì Duplikat yang dihapus: {rows_removed} baris\")\n",
    "\n",
    "# Jika ada duplikat customerID setelah drop_duplicates, hapus berdasarkan ID\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop_duplicates(subset=['customerID'], keep='first')\n",
    "    print(f\"   ‚úì Dataset setelah drop duplikat customerID: {len(df)} baris\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset cleaning complete! Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.2C PEMBERSIHAN OUTLIER FITUR NUMERIK\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PEMBERSIHAN OUTLIER FITUR NUMERIK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pilih fitur numerik (kecuali target dan ID)\n",
    "numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns if col not in ['Churn']]\n",
    "\n",
    "# Z-score method untuk deteksi outlier\n",
    "from scipy.stats import zscore\n",
    "\n",
    "outlier_indices = set()\n",
    "z_thresh = 3  # threshold z-score\n",
    "\n",
    "for col in numeric_cols:\n",
    "    z_scores = zscore(df[col])\n",
    "    outliers = np.where(np.abs(z_scores) > z_thresh)[0]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"Fitur {col}: {len(outliers)} outlier\")\n",
    "        outlier_indices.update(outliers.tolist())\n",
    "\n",
    "print(f\"Total baris outlier yang terdeteksi: {len(outlier_indices)}\")\n",
    "\n",
    "# Hapus baris outlier\n",
    "df_before = len(df)\n",
    "df = df.drop(index=list(outlier_indices)).reset_index(drop=True)\n",
    "df_after = len(df)\n",
    "print(f\"Baris sebelum: {df_before}\")\n",
    "print(f\"Baris sesudah: {df_after}\")\n",
    "print(f\"‚úì Outlier berhasil dibersihkan!\")\n",
    "\n",
    "# Cek ulang shape dataset\n",
    "print(f\"\\nShape dataset setelah cleaning outlier: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.3 VISUALISASI DISTRIBUSI TARGET (CHURN)\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALISIS VARIABEL TARGET (CHURN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Distribusi Churn\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_percentage = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nDistribusi Target Churn:\")\n",
    "print(f\"  No:  {churn_counts['No']} ({churn_percentage['No']:.2f}%)\")\n",
    "print(f\"  Yes: {churn_counts['Yes']} ({churn_percentage['Yes']:.2f}%)\")\n",
    "\n",
    "# Visualisasi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "churn_counts.plot(kind='bar', ax=axes[0], color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Distribusi Churn (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Jumlah Pelanggan')\n",
    "axes[0].set_xlabel('Churn Status')\n",
    "axes[0].set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[1].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Proporsi Churn', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cek class imbalance\n",
    "imbalance_ratio = churn_counts['Yes'] / churn_counts['No']\n",
    "print(f\"\\n‚ö†Ô∏è Class Imbalance Ratio: {imbalance_ratio:.3f}\")\n",
    "if imbalance_ratio < 0.3:\n",
    "    print(\"   Status: Dataset memiliki imbalance yang signifikan\")\n",
    "else:\n",
    "    print(\"   Status: Dataset cukup seimbang\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.4B VERSI DATASET: NORMAL, UNDERSAMPLING, OVERSAMPLING\n",
    "# ===========================\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MEMBUAT VERSI DATASET: NORMAL, UNDERSAMPLING, OVERSAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Dataset Normal (tanpa sampling)\n",
    "df_normal = df.copy()\n",
    "print(f\"\\n1Ô∏è‚É£ Dataset Normal: {df_normal.shape}\")\n",
    "print(f\"   Churn distribution:\\n{df_normal['Churn'].value_counts()}\")\n",
    "\n",
    "# Untuk sampling, perlu encode categorical terlebih dahulu\n",
    "df_temp = df_normal.copy()\n",
    "y_temp = df_temp['Churn']\n",
    "X_temp = df_temp.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "\n",
    "# Simple label encoding untuk categorical\n",
    "X_encoded = X_temp.copy()\n",
    "for col in X_encoded.columns:\n",
    "    if X_encoded[col].dtype == 'object':\n",
    "        X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col])\n",
    "\n",
    "# Handle TotalCharges jika string\n",
    "if 'TotalCharges' in X_encoded.columns and X_encoded['TotalCharges'].dtype == 'object':\n",
    "    X_encoded['TotalCharges'] = pd.to_numeric(X_encoded['TotalCharges'], errors='coerce')\n",
    "    X_encoded['TotalCharges'].fillna(X_encoded['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# 2. Dataset Undersampling\n",
    "print(\"\\n2Ô∏è‚É£ Undersampling...\")\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X_encoded, y_temp)\n",
    "df_undersample = pd.concat([X_rus.reset_index(drop=True), y_rus.reset_index(drop=True)], axis=1)\n",
    "print(f\"   Shape: {df_undersample.shape}\")\n",
    "print(f\"   Churn distribution:\\n{df_undersample['Churn'].value_counts()}\")\n",
    "\n",
    "# 3. Dataset Oversampling (SMOTE)\n",
    "print(\"\\n3Ô∏è‚É£ Oversampling (SMOTE)...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_encoded, y_temp)\n",
    "df_oversample = pd.concat([X_smote.reset_index(drop=True), y_smote.reset_index(drop=True)], axis=1)\n",
    "print(f\"   Shape: {df_oversample.shape}\")\n",
    "print(f\"   Churn distribution:\\n{df_oversample['Churn'].value_counts()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Semua versi dataset siap!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Normal:        {df_normal.shape[0]:,} rows\")\n",
    "print(f\"Undersample:   {df_undersample.shape[0]:,} rows\")\n",
    "print(f\"Oversample:    {df_oversample.shape[0]:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ad1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# A.4 ANALISIS KORELASI\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALISIS KORELASI FITUR NUMERIK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pilih fitur numerik\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nFitur Numerik yang Ditemukan: {numeric_features}\")\n",
    "\n",
    "# Buat correlation matrix\n",
    "correlation_matrix = df[numeric_features].corr()\n",
    "\n",
    "# Visualisasi heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Heatmap Korelasi Fitur Numerik', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tampilkan korelasi dengan target jika numeric\n",
    "if 'Churn' in df.columns:\n",
    "    # Encode target untuk analisis korelasi\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Churn_encoded'] = (df_temp['Churn'] == 'Yes').astype(int)\n",
    "    \n",
    "    target_correlation = df_temp[numeric_features + ['Churn_encoded']].corr()['Churn_encoded'].sort_values(ascending=False)\n",
    "    print(\"\\nKorelasi dengan Churn (Target):\")\n",
    "    print(target_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# B.4 EVALUASI MODEL UNTUK SEMUA TIPE DATASET\n",
    "# ===========================\n",
    "\n",
    "# Helper untuk encode dan split dataset balancing\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_balanced_data(df_bal):\n",
    "    df_bal = shuffle(df_bal, random_state=42).reset_index(drop=True)\n",
    "    y = (df_bal['Churn'] == 'Yes').astype(int) if df_bal['Churn'].dtype == 'object' else df_bal['Churn']\n",
    "    X = df_bal.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "    # Simple encoding categorical\n",
    "    X_encoded = X.copy()\n",
    "    for col in X_encoded.columns:\n",
    "        if X_encoded[col].dtype == 'object':\n",
    "            X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col])\n",
    "    # Handle TotalCharges\n",
    "    if 'TotalCharges' in X_encoded.columns and X_encoded['TotalCharges'].dtype == 'object':\n",
    "        X_encoded['TotalCharges'] = pd.to_numeric(X_encoded['TotalCharges'], errors='coerce')\n",
    "        X_encoded['TotalCharges'].fillna(X_encoded['TotalCharges'].median(), inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# List dataset balancing\n",
    "datasets = [\n",
    "    (\"Normal\", df_normal),\n",
    "    (\"Undersampling\", df_undersample),\n",
    "    (\"Oversampling\", df_oversample)\n",
    "]\n",
    "\n",
    "results_balanced = []\n",
    "for ds_name, ds in datasets:\n",
    "    X_train, X_test, y_train, y_test = prepare_balanced_data(ds)\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    res_lr = evaluate_model(lr, X_test, y_test, f\"Logistic Regression (Direct) ({ds_name})\")\n",
    "    results_balanced.append(res_lr)\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    res_rf = evaluate_model(rf, X_test, y_test, f\"Random Forest (Direct) ({ds_name})\")\n",
    "    results_balanced.append(res_rf)\n",
    "    # Voting Classifier\n",
    "    voting = VotingClassifier([\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "        ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "    ], voting='soft')\n",
    "    voting.fit(X_train, y_train)\n",
    "    res_voting = evaluate_model(voting, X_test, y_test, f\"Voting Classifier (Direct) ({ds_name})\")\n",
    "    results_balanced.append(res_voting)\n",
    "\n",
    "# Gabungkan hasil\n",
    "all_results_balanced = pd.DataFrame(results_balanced)\n",
    "print(\"\\n\\nüìä RINGKASAN HASIL SEMUA DATASET BALANCING:\")\n",
    "print(all_results_balanced[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85425893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# B.5 EVALUASI MODEL PREPROCESSING & TUNED UNTUK SEMUA TIPE DATASET\n",
    "# ===========================\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Hyperparameter grids\n",
    "lr_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "results_all = []\n",
    "for ds_name, ds in datasets:\n",
    "    # --- Preprocessing ---\n",
    "    df_prep = ds.copy()\n",
    "    y = (df_prep['Churn'] == 'Yes').astype(int) if df_prep['Churn'].dtype == 'object' else df_prep['Churn']\n",
    "    X = df_prep.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "    # Identify categorical columns\n",
    "    cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if 'TotalCharges' in X.columns and X['TotalCharges'].dtype == 'object':\n",
    "        X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "        X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "        num_cols.append('TotalCharges')\n",
    "        if 'TotalCharges' in cat_cols:\n",
    "            cat_cols.remove('TotalCharges')\n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Logistic Regression (Preprocessing)\n",
    "    lr_pipe = make_pipeline(preprocessor, LogisticRegression(random_state=42, max_iter=1000))\n",
    "    lr_pipe.fit(X_train, y_train)\n",
    "    res_lr_prep = evaluate_model(lr_pipe, X_test, y_test, f\"Logistic Regression (Preprocessing) ({ds_name})\")\n",
    "    results_all.append(res_lr_prep)\n",
    "    # Random Forest (Preprocessing)\n",
    "    rf_pipe = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    res_rf_prep = evaluate_model(rf_pipe, X_test, y_test, f\"Random Forest (Preprocessing) ({ds_name})\")\n",
    "    results_all.append(res_rf_prep)\n",
    "    # Voting Classifier (Preprocessing)\n",
    "    voting_pipe = make_pipeline(preprocessor, VotingClassifier([\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "        ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "    ], voting='soft'))\n",
    "    voting_pipe.fit(X_train, y_train)\n",
    "    res_voting_prep = evaluate_model(voting_pipe, X_test, y_test, f\"Voting Classifier (Preprocessing) ({ds_name})\")\n",
    "    results_all.append(res_voting_prep)\n",
    "\n",
    "    # --- Tuned ---\n",
    "    # Logistic Regression (Tuned)\n",
    "    lr_grid = make_pipeline(preprocessor, GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), lr_params, cv=5, scoring='f1', n_jobs=-1))\n",
    "    lr_grid.fit(X_train, y_train)\n",
    "    best_lr = lr_grid.named_steps['gridsearchcv'].best_estimator_\n",
    "    tuned_lr_pipe = make_pipeline(preprocessor, best_lr)\n",
    "    tuned_lr_pipe.fit(X_train, y_train)\n",
    "    res_lr_tuned = evaluate_model(tuned_lr_pipe, X_test, y_test, f\"Logistic Regression (Tuned) ({ds_name})\")\n",
    "    results_all.append(res_lr_tuned)\n",
    "    # Random Forest (Tuned)\n",
    "    rf_grid = make_pipeline(preprocessor, GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='f1', n_jobs=-1))\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    best_rf = rf_grid.named_steps['gridsearchcv'].best_estimator_\n",
    "    tuned_rf_pipe = make_pipeline(preprocessor, best_rf)\n",
    "    tuned_rf_pipe.fit(X_train, y_train)\n",
    "    res_rf_tuned = evaluate_model(tuned_rf_pipe, X_test, y_test, f\"Random Forest (Tuned) ({ds_name})\")\n",
    "    results_all.append(res_rf_tuned)\n",
    "    # Voting Classifier (Tuned) (base estimators tuned)\n",
    "    voting_tuned = VotingClassifier([\n",
    "        ('lr', LogisticRegression(C=1, random_state=42, max_iter=1000)),\n",
    "        ('svm', SVC(kernel='rbf', C=1, gamma='scale', random_state=42, probability=True)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "    ], voting='soft')\n",
    "    voting_tuned_pipe = make_pipeline(preprocessor, voting_tuned)\n",
    "    voting_tuned_pipe.fit(X_train, y_train)\n",
    "    res_voting_tuned = evaluate_model(voting_tuned_pipe, X_test, y_test, f\"Voting Classifier (Tuned) ({ds_name})\")\n",
    "    results_all.append(res_voting_tuned)\n",
    "\n",
    "# Gabungkan semua hasil\n",
    "all_results_full = pd.concat([all_results_balanced, pd.DataFrame(results_all)], ignore_index=True)\n",
    "print(\"\\n\\nüìä RINGKASAN HASIL SEMUA MODEL DAN DATASET:\")\n",
    "print(all_results_full[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30591af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# C.1 DATA PREPROCESSING\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_prep = df.copy()\n",
    "\n",
    "# 1. Handle TotalCharges (convert to numeric)\n",
    "print(\"\\n1Ô∏è‚É£ Handling TotalCharges...\")\n",
    "df_prep['TotalCharges'] = pd.to_numeric(df_prep['TotalCharges'], errors='coerce')\n",
    "df_prep['TotalCharges'].fillna(df_prep['TotalCharges'].median(), inplace=True)\n",
    "print(\"   ‚úì TotalCharges converted to numeric\")\n",
    "\n",
    "# 2. Drop tidak relevan columns\n",
    "print(\"\\n2Ô∏è‚É£ Dropping irrelevant columns...\")\n",
    "df_prep = df_prep.drop(['customerID'], axis=1)\n",
    "print(\"   ‚úì customerID dropped\")\n",
    "\n",
    "# 3. Encode target\n",
    "print(\"\\n3Ô∏è‚É£ Encoding target variable...\")\n",
    "df_prep['Churn'] = (df_prep['Churn'] == 'Yes').astype(int)\n",
    "print(\"   ‚úì Churn encoded\")\n",
    "\n",
    "# 4. Pisahkan numeric dan categorical\n",
    "print(\"\\n4Ô∏è‚É£ Separating numeric and categorical features...\")\n",
    "numeric_cols = df_prep.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df_prep.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"   Numeric columns: {numeric_cols}\")\n",
    "print(f\"   Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# 5. One-Hot Encoding untuk categorical\n",
    "print(\"\\n5Ô∏è‚É£ Applying One-Hot Encoding...\")\n",
    "df_prep_encoded = pd.get_dummies(df_prep, columns=categorical_cols, drop_first=True)\n",
    "print(f\"   ‚úì Features after encoding: {df_prep_encoded.shape[1]}\")\n",
    "\n",
    "# 6. Pisahkan X dan y\n",
    "y_prep = df_prep_encoded['Churn']\n",
    "X_prep = df_prep_encoded.drop(['Churn'], axis=1)\n",
    "\n",
    "# 7. Scaling\n",
    "print(\"\\n6Ô∏è‚É£ Feature Scaling (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "X_prep_scaled = scaler.fit_transform(X_prep)\n",
    "X_prep_scaled = pd.DataFrame(X_prep_scaled, columns=X_prep.columns)\n",
    "print(\"   ‚úì Features scaled\")\n",
    "\n",
    "# Train-test split\n",
    "X_train_prep, X_test_prep, y_train_prep, y_test_prep = train_test_split(\n",
    "    X_prep_scaled, y_prep, test_size=0.2, random_state=42, stratify=y_prep\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")\n",
    "print(f\"  Training set: {X_train_prep.shape}\")\n",
    "print(f\"  Testing set: {X_test_prep.shape}\")\n",
    "print(f\"  Total features: {X_train_prep.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f60b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# C.2 TRAINING MODELS WITH PREPROCESSING (INTEGRATED)\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODELS WITH PREPROCESSING (DIINTEGRASIKAN DI EVALUASI LENGKAP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining untuk Logistic Regression, Random Forest, dan Voting Classifier\")\n",
    "print(\"dilakukan per-dataset (Normal/Undersampling/Oversampling) di bagian B.5\")\n",
    "print(\"menggunakan pipeline (ColumnTransformer + StandardScaler + OneHotEncoder).\")\n",
    "print(\"Cell ini tidak melakukan training langsung untuk menghindari duplikasi.\")\n",
    "\n",
    "print(\"\\nReferensi konfigurasi base models:\")\n",
    "print(\" - Logistic Regression: random_state=42, max_iter=1000\")\n",
    "print(\" - Random Forest: n_estimators=100, random_state=42\")\n",
    "print(\" - Voting Classifier: LR + SVM(rbf) + KNN, voting='soft'\")\n",
    "\n",
    "print(\"\\n‚úì Silakan lihat B.5 untuk proses training lintas dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# C.3 EVALUASI MODEL (PREPROCESSING - SEMUA DATASET)\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HASIL EVALUASI - PREPROCESSING ACROSS ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Gunakan hasil lengkap jika tersedia; kalau tidak, hitung bagian preprocessing saja\n",
    "try:\n",
    "    _ = all_results_full\n",
    "    prep_results = all_results_full[all_results_full['Model'].str.contains('(Preprocessing)', regex=False)].copy()\n",
    "    print(\"\\n‚úì Menggunakan hasil preprocessing dari evaluasi lengkap (27 kombinasi).\")\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è all_results_full belum tersedia; menghitung ulang hasil preprocessing untuk semua dataset...\")\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    # Siapkan dataset jika belum ada\n",
    "    try:\n",
    "        _ = df_normal\n",
    "    except NameError:\n",
    "        df_normal = df.copy()\n",
    "    try:\n",
    "        _ = df_undersample\n",
    "        _ = df_oversample\n",
    "    except NameError:\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_rus, y_rus = rus.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_undersample = pd.concat([X_rus, y_rus], axis=1)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_smote, y_smote = smote.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_oversample = pd.concat([X_smote, y_smote], axis=1)\n",
    "\n",
    "    datasets = [\n",
    "        (\"Normal\", df_normal),\n",
    "        (\"Undersampling\", df_undersample),\n",
    "        (\"Oversampling\", df_oversample)\n",
    "    ]\n",
    "\n",
    "    prep_rows = []\n",
    "    for ds_name, ds in datasets:\n",
    "        df_prep = ds.copy()\n",
    "        y = (df_prep['Churn'] == 'Yes').astype(int) if df_prep['Churn'].dtype == 'object' else df_prep['Churn']\n",
    "        X = df_prep.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "\n",
    "        cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        if 'TotalCharges' in X.columns and X['TotalCharges'].dtype == 'object':\n",
    "            X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "            X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "            if 'TotalCharges' not in num_cols:\n",
    "                num_cols.append('TotalCharges')\n",
    "            if 'TotalCharges' in cat_cols:\n",
    "                cat_cols.remove('TotalCharges')\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        # Logistic Regression (Preprocessing)\n",
    "        lr_pipe = make_pipeline(preprocessor, LogisticRegression(random_state=42, max_iter=1000))\n",
    "        lr_pipe.fit(X_train, y_train)\n",
    "        prep_rows.append(evaluate_model(lr_pipe, X_test, y_test, f\"Logistic Regression (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "        # Random Forest (Preprocessing)\n",
    "        rf_pipe = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        rf_pipe.fit(X_train, y_train)\n",
    "        prep_rows.append(evaluate_model(rf_pipe, X_test, y_test, f\"Random Forest (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "        # Voting Classifier (Preprocessing)\n",
    "        voting_pipe = make_pipeline(preprocessor, VotingClassifier([\n",
    "            ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "        ], voting='soft'))\n",
    "        voting_pipe.fit(X_train, y_train)\n",
    "        prep_rows.append(evaluate_model(voting_pipe, X_test, y_test, f\"Voting Classifier (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "    prep_results = pd.DataFrame(prep_rows)\n",
    "\n",
    "# Ringkasan hasil preprocessing\n",
    "prep_sorted = prep_results.sort_values('F1-Score', ascending=False)\n",
    "print(\"\\n\\nüìä RINGKASAN HASIL PREPROCESSING (3 model √ó 3 dataset):\")\n",
    "print(prep_sorted[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# Visualisasi perbandingan\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    ax.bar(prep_sorted['Model'], prep_sorted[metric], color=['#3498db']*len(prep_sorted))\n",
    "    ax.set_title(f'{metric} - Preprocessing (All Datasets)', fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    for i, v in enumerate(prep_sorted[metric]):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# D.1 HYPERPARAMETER TUNING (INTEGRATED)\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING (DIINTEGRASIKAN KE EVALUASI LENGKAP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTuning dilakukan per-dataset di bagian evaluasi lengkap (B.5) \")\n",
    "print(\"menggunakan pipeline + GridSearchCV untuk Logistic Regression dan Random Forest.\")\n",
    "print(\"Ringkasan hasil tuned lintas dataset tersedia di bagian D.2.\")\n",
    "\n",
    "# Untuk konsistensi dan kecepatan saat fallback, siapkan konfigurasi tuned yang dipakai:\n",
    "print(\"\\nKonfigurasi tuned yang digunakan (fallback cepat):\")\n",
    "print(\" - Logistic Regression: C=1, penalty='l2', solver='lbfgs'\")\n",
    "print(\" - Random Forest: n_estimators=200, max_depth=20, random_state=42\")\n",
    "print(\" - Voting Classifier: LR(C=1), SVM(rbf, C=1, gamma='scale'), KNN(n_neighbors=7)\")\n",
    "\n",
    "# Siapkan objek model tuned (dapat dipakai ulang jika diperlukan)\n",
    "lr_tuned = LogisticRegression(C=1, random_state=42, max_iter=1000)\n",
    "rf_tuned = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "voting_tuned = VotingClassifier([\n",
    "    ('lr', LogisticRegression(C=1, random_state=42, max_iter=1000)),\n",
    "    ('svm', SVC(kernel='rbf', C=1, gamma='scale', random_state=42, probability=True)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "], voting='soft')\n",
    "\n",
    "print(\"\\n‚úì Konfigurasi tuned siap. Lanjutkan ke D.2 untuk ringkasan hasil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af970068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# D.2 EVALUASI MODEL TUNED (SEMUA DATASET)\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HASIL EVALUASI - TUNED ACROSS ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Jika tersedia, gunakan hasil lengkap; kalau tidak, hitung khusus bagian tuned\n",
    "try:\n",
    "    _ = all_results_full\n",
    "    tuned_results = all_results_full[all_results_full['Model'].str.contains('(Tuned)', regex=False)].copy()\n",
    "    print(\"\\n‚úì Menggunakan hasil tuned dari evaluasi lengkap (27 kombinasi).\")\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è all_results_full belum tersedia; menghitung ulang hasil tuned untuk semua dataset...\")\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    # Siapkan dataset jika belum ada\n",
    "    try:\n",
    "        _ = df_normal\n",
    "    except NameError:\n",
    "        df_normal = df.copy()\n",
    "    try:\n",
    "        _ = df_undersample\n",
    "        _ = df_oversample\n",
    "    except NameError:\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_rus, y_rus = rus.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_undersample = pd.concat([X_rus, y_rus], axis=1)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_smote, y_smote = smote.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_oversample = pd.concat([X_smote, y_smote], axis=1)\n",
    "\n",
    "    datasets = [\n",
    "        (\"Normal\", df_normal),\n",
    "        (\"Undersampling\", df_undersample),\n",
    "        (\"Oversampling\", df_oversample)\n",
    "    ]\n",
    "\n",
    "    tuned_rows = []\n",
    "    for ds_name, ds in datasets:\n",
    "        df_prep = ds.copy()\n",
    "        y = (df_prep['Churn'] == 'Yes').astype(int) if df_prep['Churn'].dtype == 'object' else df_prep['Churn']\n",
    "        X = df_prep.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "\n",
    "        cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        if 'TotalCharges' in X.columns and X['TotalCharges'].dtype == 'object':\n",
    "            X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "            X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "            if 'TotalCharges' not in num_cols:\n",
    "                num_cols.append('TotalCharges')\n",
    "            if 'TotalCharges' in cat_cols:\n",
    "                cat_cols.remove('TotalCharges')\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        # Tuned Logistic Regression\n",
    "        lr_tuned = LogisticRegression(C=1, random_state=42, max_iter=1000)\n",
    "        lr_tuned_pipe = make_pipeline(preprocessor, lr_tuned)\n",
    "        lr_tuned_pipe.fit(X_train, y_train)\n",
    "        tuned_rows.append(evaluate_model(lr_tuned_pipe, X_test, y_test, f\"Logistic Regression (Tuned) ({ds_name})\"))\n",
    "\n",
    "        # Tuned Random Forest\n",
    "        rf_tuned = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "        rf_tuned_pipe = make_pipeline(preprocessor, rf_tuned)\n",
    "        rf_tuned_pipe.fit(X_train, y_train)\n",
    "        tuned_rows.append(evaluate_model(rf_tuned_pipe, X_test, y_test, f\"Random Forest (Tuned) ({ds_name})\"))\n",
    "\n",
    "        # Tuned Voting Classifier\n",
    "        voting_tuned = VotingClassifier([\n",
    "            ('lr', LogisticRegression(C=1, random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', C=1, gamma='scale', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "        ], voting='soft')\n",
    "        voting_tuned_pipe = make_pipeline(preprocessor, voting_tuned)\n",
    "        voting_tuned_pipe.fit(X_train, y_train)\n",
    "        tuned_rows.append(evaluate_model(voting_tuned_pipe, X_test, y_test, f\"Voting Classifier (Tuned) ({ds_name})\"))\n",
    "\n",
    "    tuned_results = pd.DataFrame(tuned_rows)\n",
    "\n",
    "# Ringkasan hasil tuned\n",
    "tuned_sorted = tuned_results.sort_values('F1-Score', ascending=False)\n",
    "print(\"\\n\\nüìä RINGKASAN HASIL TUNED (3 model √ó 3 dataset):\")\n",
    "print(tuned_sorted[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# Visualisasi perbandingan\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    ax.bar(tuned_sorted['Model'], tuned_sorted[metric], color=['#9b59b6']*len(tuned_sorted))\n",
    "    ax.set_title(f'{metric} - Tuned (All Datasets)', fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    for i, v in enumerate(tuned_sorted[metric]):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485caeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# E. COMPREHENSIVE COMPARISON\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERBANDINGAN SEMUA 27 MODEL (3 model √ó 3 tipe √ó 3 dataset)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pastikan all_results_full tersedia; kalau belum, hitung ulang 27 hasil secara otomatis\n",
    "try:\n",
    "    _ = all_results_full\n",
    "    all_results = all_results_full.copy()\n",
    "    print(\"\\n‚úì Menggunakan hasil evaluasi lengkap yang sudah ada.\")\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è all_results_full belum tersedia; menghitung ulang semua 27 kombinasi...\")\n",
    "    from sklearn.utils import shuffle\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    # Siapkan dataset jika belum ada\n",
    "    try:\n",
    "        _ = df_normal\n",
    "    except NameError:\n",
    "        df_normal = df.copy()\n",
    "    try:\n",
    "        _ = df_undersample\n",
    "        _ = df_oversample\n",
    "    except NameError:\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_rus, y_rus = rus.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_undersample = pd.concat([X_rus, y_rus], axis=1)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_smote, y_smote = smote.fit_resample(df_normal.drop('Churn', axis=1), df_normal['Churn'])\n",
    "        df_oversample = pd.concat([X_smote, y_smote], axis=1)\n",
    "\n",
    "    datasets = [\n",
    "        (\"Normal\", df_normal),\n",
    "        (\"Undersampling\", df_undersample),\n",
    "        (\"Oversampling\", df_oversample)\n",
    "    ]\n",
    "\n",
    "    def prepare_balanced_data(df_bal):\n",
    "        df_bal = shuffle(df_bal, random_state=42).reset_index(drop=True)\n",
    "        y = (df_bal['Churn'] == 'Yes').astype(int) if df_bal['Churn'].dtype == 'object' else df_bal['Churn']\n",
    "        X = df_bal.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "        X_encoded = X.copy()\n",
    "        for col in X_encoded.columns:\n",
    "            if X_encoded[col].dtype == 'object':\n",
    "                X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col])\n",
    "        if 'TotalCharges' in X_encoded.columns and X_encoded['TotalCharges'].dtype == 'object':\n",
    "            X_encoded['TotalCharges'] = pd.to_numeric(X_encoded['TotalCharges'], errors='coerce')\n",
    "            X_encoded['TotalCharges'].fillna(X_encoded['TotalCharges'].median(), inplace=True)\n",
    "        return train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # 9 hasil untuk Direct across datasets\n",
    "    results_balanced = []\n",
    "    for ds_name, ds in datasets:\n",
    "        X_train, X_test, y_train, y_test = prepare_balanced_data(ds)\n",
    "        lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr.fit(X_train, y_train)\n",
    "        results_balanced.append(evaluate_model(lr, X_test, y_test, f\"Logistic Regression (Direct) ({ds_name})\"))\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        results_balanced.append(evaluate_model(rf, X_test, y_test, f\"Random Forest (Direct) ({ds_name})\"))\n",
    "        voting = VotingClassifier([\n",
    "            ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "        ], voting='soft')\n",
    "        voting.fit(X_train, y_train)\n",
    "        results_balanced.append(evaluate_model(voting, X_test, y_test, f\"Voting Classifier (Direct) ({ds_name})\"))\n",
    "\n",
    "    all_results_balanced = pd.DataFrame(results_balanced)\n",
    "\n",
    "    # 18 hasil untuk Preprocessing & Tuned across datasets\n",
    "    results_all = []\n",
    "    for ds_name, ds in datasets:\n",
    "        df_prep = ds.copy()\n",
    "        y = (df_prep['Churn'] == 'Yes').astype(int) if df_prep['Churn'].dtype == 'object' else df_prep['Churn']\n",
    "        X = df_prep.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "        cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        if 'TotalCharges' in X.columns and X['TotalCharges'].dtype == 'object':\n",
    "            X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "            X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "            if 'TotalCharges' not in num_cols:\n",
    "                num_cols.append('TotalCharges')\n",
    "            if 'TotalCharges' in cat_cols:\n",
    "                cat_cols.remove('TotalCharges')\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        lr_pipe = make_pipeline(preprocessor, LogisticRegression(random_state=42, max_iter=1000))\n",
    "        lr_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(lr_pipe, X_test, y_test, f\"Logistic Regression (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "        rf_pipe = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        rf_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(rf_pipe, X_test, y_test, f\"Random Forest (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "        voting_pipe = make_pipeline(preprocessor, VotingClassifier([\n",
    "            ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "        ], voting='soft'))\n",
    "        voting_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(voting_pipe, X_test, y_test, f\"Voting Classifier (Preprocessing) ({ds_name})\"))\n",
    "\n",
    "        # Tuned variants (simple fixed best params for speed)\n",
    "        lr_tuned = LogisticRegression(C=1, random_state=42, max_iter=1000)\n",
    "        lr_tuned_pipe = make_pipeline(preprocessor, lr_tuned)\n",
    "        lr_tuned_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(lr_tuned_pipe, X_test, y_test, f\"Logistic Regression (Tuned) ({ds_name})\"))\n",
    "\n",
    "        rf_tuned = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "        rf_tuned_pipe = make_pipeline(preprocessor, rf_tuned)\n",
    "        rf_tuned_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(rf_tuned_pipe, X_test, y_test, f\"Random Forest (Tuned) ({ds_name})\"))\n",
    "\n",
    "        voting_tuned = VotingClassifier([\n",
    "            ('lr', LogisticRegression(C=1, random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', C=1, gamma='scale', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "        ], voting='soft')\n",
    "        voting_tuned_pipe = make_pipeline(preprocessor, voting_tuned)\n",
    "        voting_tuned_pipe.fit(X_train, y_train)\n",
    "        results_all.append(evaluate_model(voting_tuned_pipe, X_test, y_test, f\"Voting Classifier (Tuned) ({ds_name})\"))\n",
    "\n",
    "    all_results = pd.concat([all_results_balanced, pd.DataFrame(results_all)], ignore_index=True)\n",
    "    # Simpan ke all_results_full untuk dipakai cell lain\n",
    "    all_results_full = all_results.copy()\n",
    "    print(\"‚úì Selesai menghitung ulang semua 27 kombinasi.\")\n",
    "\n",
    "# Sorting berdasarkan F1-Score\n",
    "all_results_sorted = all_results.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä RANKING SEMUA MODEL:\")\n",
    "print(all_results_sorted[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# Identifikasi best model\n",
    "best_model_idx = all_results['F1-Score'].idxmax()\n",
    "best_model_info = all_results.loc[best_model_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÜ BEST MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model: {best_model_info['Model']}\")\n",
    "print(f\"Accuracy:  {best_model_info['Accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_model_info['Precision']:.4f}\")\n",
    "print(f\"Recall:    {best_model_info['Recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_model_info['F1-Score']:.4f}\")\n",
    "\n",
    "# Visualisasi comparison\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "x = np.arange(len(all_results_sorted))\n",
    "width = 0.2\n",
    "\n",
    "metrics_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors_plot = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, metric in enumerate(metrics_plot):\n",
    "    ax.bar(x + i*width, all_results_sorted[metric], width, label=metric, color=colors_plot[i])\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Perbandingan Performa Semua 27 Model', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(all_results_sorted['Model'], rotation=90, ha='right', fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 16))\n",
    "comparison_data = all_results_sorted[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']].set_index('Model')\n",
    "sns.heatmap(comparison_data, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Score'})\n",
    "ax.set_title('Heatmap Performa Semua Model', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252dd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# F.1 SAVE BEST MODEL\n",
    "# ===========================\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING BEST MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pilih best model berdasarkan F1-Score tertinggi dari all_results_full\n",
    "best_idx = all_results_full['F1-Score'].idxmax()\n",
    "best_model_info = all_results_full.loc[best_idx]\n",
    "best_model_name = best_model_info['Model']\n",
    "\n",
    "print(f\"\\nüèÜ Model terbaik berdasarkan F1-Score: {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_model_info['F1-Score']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model_info['Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {best_model_info['Precision']:.4f}\")\n",
    "print(f\"   Recall: {best_model_info['Recall']:.4f}\")\n",
    "\n",
    "# Parse model name untuk menentukan model, tipe, dan dataset\n",
    "# Format: \"Model Name (Type) (Dataset)\"\n",
    "# Contoh: \"Random Forest (Tuned) (Normal)\"\n",
    "\n",
    "# Extract model type dan dataset type dari nama\n",
    "if \"Logistic Regression\" in best_model_name:\n",
    "    model_type = \"Logistic Regression\"\n",
    "elif \"Random Forest\" in best_model_name:\n",
    "    model_type = \"Random Forest\"\n",
    "elif \"Voting Classifier\" in best_model_name:\n",
    "    model_type = \"Voting Classifier\"\n",
    "else:\n",
    "    model_type = \"Unknown\"\n",
    "\n",
    "if \"(Direct)\" in best_model_name:\n",
    "    processing_type = \"Direct\"\n",
    "elif \"(Preprocessing)\" in best_model_name:\n",
    "    processing_type = \"Preprocessing\"\n",
    "elif \"(Tuned)\" in best_model_name:\n",
    "    processing_type = \"Tuned\"\n",
    "else:\n",
    "    processing_type = \"Unknown\"\n",
    "\n",
    "if \"(Normal)\" in best_model_name:\n",
    "    dataset_type = \"Normal\"\n",
    "elif \"(Undersampling)\" in best_model_name:\n",
    "    dataset_type = \"Undersampling\"\n",
    "elif \"(Oversampling)\" in best_model_name:\n",
    "    dataset_type = \"Oversampling\"\n",
    "else:\n",
    "    dataset_type = \"Normal\"\n",
    "\n",
    "print(f\"\\nüìã Konfigurasi Model:\")\n",
    "print(f\"   Model: {model_type}\")\n",
    "print(f\"   Processing: {processing_type}\")\n",
    "print(f\"   Dataset: {dataset_type}\")\n",
    "\n",
    "# Pilih dataset yang sesuai\n",
    "if dataset_type == \"Normal\":\n",
    "    df_selected = df_normal\n",
    "elif dataset_type == \"Undersampling\":\n",
    "    df_selected = df_undersample\n",
    "elif dataset_type == \"Oversampling\":\n",
    "    df_selected = df_oversample\n",
    "else:\n",
    "    df_selected = df_normal\n",
    "\n",
    "# Retrain best model configuration\n",
    "print(f\"\\nüîß Retraining model dengan konfigurasi terbaik...\")\n",
    "\n",
    "if processing_type == \"Direct\":\n",
    "    # Direct: Simple encoding\n",
    "    df_train = df_selected.copy()\n",
    "    y = (df_train['Churn'] == 'Yes').astype(int) if df_train['Churn'].dtype == 'object' else df_train['Churn']\n",
    "    X = df_train.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "    X_encoded = X.copy()\n",
    "    for col in X_encoded.columns:\n",
    "        if X_encoded[col].dtype == 'object':\n",
    "            X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col])\n",
    "    if 'TotalCharges' in X_encoded.columns and X_encoded['TotalCharges'].dtype == 'object':\n",
    "        X_encoded['TotalCharges'] = pd.to_numeric(X_encoded['TotalCharges'], errors='coerce')\n",
    "        X_encoded['TotalCharges'].fillna(X_encoded['TotalCharges'].median(), inplace=True)\n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "        X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    best_scaler = None\n",
    "    feature_names_final = X_train_final.columns.tolist()\n",
    "    \n",
    "else:\n",
    "    # Preprocessing or Tuned: Full pipeline\n",
    "    df_train = df_selected.copy()\n",
    "    y = (df_train['Churn'] == 'Yes').astype(int) if df_train['Churn'].dtype == 'object' else df_train['Churn']\n",
    "    X = df_train.drop(['Churn', 'customerID'], axis=1, errors='ignore')\n",
    "    \n",
    "    cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    if 'TotalCharges' in X.columns and X['TotalCharges'].dtype == 'object':\n",
    "        X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "        X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "        if 'TotalCharges' not in num_cols:\n",
    "            num_cols.append('TotalCharges')\n",
    "        if 'TotalCharges' in cat_cols:\n",
    "            cat_cols.remove('TotalCharges')\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "    \n",
    "    X_train_split, X_test_split, y_train_final, y_test_final = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train_final = preprocessor.fit_transform(X_train_split)\n",
    "    X_test_final = preprocessor.transform(X_test_split)\n",
    "    \n",
    "    best_scaler = preprocessor\n",
    "    # Get feature names after preprocessing\n",
    "    num_feature_names = num_cols\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols).tolist()\n",
    "    feature_names_final = num_feature_names + cat_feature_names\n",
    "\n",
    "# Train final model\n",
    "if model_type == \"Logistic Regression\":\n",
    "    if processing_type == \"Tuned\":\n",
    "        best_model_final = LogisticRegression(C=1, random_state=42, max_iter=1000)\n",
    "    else:\n",
    "        best_model_final = LogisticRegression(random_state=42, max_iter=1000)\n",
    "elif model_type == \"Random Forest\":\n",
    "    if processing_type == \"Tuned\":\n",
    "        best_model_final = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "    else:\n",
    "        best_model_final = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "elif model_type == \"Voting Classifier\":\n",
    "    if processing_type == \"Tuned\":\n",
    "        best_model_final = VotingClassifier([\n",
    "            ('lr', LogisticRegression(C=1, random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', C=1, gamma='scale', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "        ], voting='soft')\n",
    "    else:\n",
    "        best_model_final = VotingClassifier([\n",
    "            ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "            ('svm', SVC(kernel='rbf', random_state=42, probability=True)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "        ], voting='soft')\n",
    "\n",
    "best_model_final.fit(X_train_final, y_train_final)\n",
    "print(f\"   ‚úì Model retrained successfully\")\n",
    "\n",
    "# Verify model performance\n",
    "y_pred_final = best_model_final.predict(X_test_final)\n",
    "final_f1 = f1_score(y_test_final, y_pred_final)\n",
    "print(f\"   ‚úì Verification F1-Score: {final_f1:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"best_churn_model.pkl\"\n",
    "joblib.dump(best_model_final, model_path)\n",
    "print(f\"\\n‚úì Model saved: {model_path}\")\n",
    "\n",
    "# Save scaler/preprocessor\n",
    "scaler_path = \"scaler.pkl\"\n",
    "if best_scaler is not None:\n",
    "    joblib.dump(best_scaler, scaler_path)\n",
    "    print(f\"‚úì Preprocessor saved: {scaler_path}\")\n",
    "else:\n",
    "    print(f\"‚úì No preprocessor needed (Direct encoding)\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = \"feature_names.pkl\"\n",
    "joblib.dump(feature_names_final, feature_names_path)\n",
    "print(f\"‚úì Feature names saved: {feature_names_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': model_type,\n",
    "    'processing_type': processing_type,\n",
    "    'dataset_type': dataset_type,\n",
    "    'f1_score': best_model_info['F1-Score'],\n",
    "    'accuracy': best_model_info['Accuracy'],\n",
    "    'precision': best_model_info['Precision'],\n",
    "    'recall': best_model_info['Recall'],\n",
    "    'trained_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "metadata_path = \"model_metadata.pkl\"\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"‚úì Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEPLOYMENT FILES READY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest Model Configuration:\")\n",
    "print(f\"  üìå {best_model_name}\")\n",
    "print(f\"  üìä F1-Score: {best_model_info['F1-Score']:.4f}\")\n",
    "print(f\"\\nFiles untuk Streamlit deployment:\")\n",
    "print(f\"  1. {model_path}\")\n",
    "print(f\"  2. {scaler_path}\")\n",
    "print(f\"  3. {feature_names_path}\")\n",
    "print(f\"  4. {metadata_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
